batch_size: 32
epochs: 1000
eval_every_n_epochs: 5
fine_tune_from: Jan16_02-27-36_edu-GPU-Linux
log_every_n_steps: 2
learning_rate: 1e-4
weight_decay: 1e-6
fp16_precision: True
truncation: True

model:
  out_dim: 512
  res_base_model: "resnet50"
  bert_base_model: 'emilyalsentzer/Bio_ClinicalBERT' #'sentence-transformers/paraphrase-xlm-r-multilingual-v1'
  freeze_layers: [0,1,2,3,4,5]
  do_lower_case: False
  
dataset:
  s: 1
  input_shape: (300,300,3)
  num_workers: 4
  valid_size: 0.1
  csv_file: '../data/creating_dataset/clr_dataset.csv' # Add the MIMIC-CXR path here 
  img_root_dir: '../../../SSD/cont_learning_data/images' # 

loss:
  temperature: 0.1
  use_cosine_similarity: True
  alpha_weight: 0.75

### BERT Models
# emilyalsentzer/Bio_ClinicalBERT A melhor!!!
# bert-base-uncased
# neuralmind/bert-base-portuguese-cased
# distilbert-base-nli-mean-tokens
# distilbert-base-multilingual-cased
# distiluse-base-multilingual-cased-v2
# sentence-transformers/distilbert-base-nli-stsb-mean-tokens
# sentence-transformers/xlm-r-100langs-bert-base-nli-stsb-mean-tokens funcionou bem
# cross-encoder/stsb-roberta-base
# sentence-transformers/paraphrase-xlm-r-multilingual-v1 # Muito Boa!